{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3705663",
   "metadata": {},
   "source": [
    "# Домашнее задание № 2. Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf72d19",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045e99",
   "metadata": {},
   "source": [
    "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способо заменить её на кастомную токенизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4d453",
   "metadata": {},
   "source": [
    "Обучите векторайзер с дефолтной токенизацией и с токенизацией razdel.tokenize. Обучите классификатор с каждым из векторизаторов. Сравните метрики и выберете победителя. \n",
    "\n",
    "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "129c4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f04b3803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\123ha\\\\comp ling'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d282f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raztok(comment):\n",
    "    list(razdel_tokenize(comment))\n",
    "    return [token.text for token in list(razdel_tokenize(comment))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bbc2365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>raztokd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Верблюдов-то, за, что, ?, Дебилы, ,, бл, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Хохлы, ,, это, отдушина, затюканого, россияни...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Собаке, -, собачья, смерть]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Страницу, обнови, ,, дебил, ., Это, тоже, не,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[тебя, не, убедил, 6-страничный, пдф, в, том, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic  \\\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0   \n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0   \n",
       "2                          Собаке - собачья смерть\\n    1.0   \n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0   \n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0   \n",
       "\n",
       "                                             raztokd  \n",
       "0     [Верблюдов-то, за, что, ?, Дебилы, ,, бл, ...]  \n",
       "1  [Хохлы, ,, это, отдушина, затюканого, россияни...  \n",
       "2                       [Собаке, -, собачья, смерть]  \n",
       "3  [Страницу, обнови, ,, дебил, ., Это, тоже, не,...  \n",
       "4  [тебя, не, убедил, 6-страничный, пдф, в, том, ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('labeled.csv')\n",
    "data['raztokd'] = data.apply(lambda row: raztok(row['comment']), axis =1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01341538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код\n",
    "train, test = train_test_split(data, test_size = 0.1, shuffle = True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3df99cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment)\n",
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a0569ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.82      0.87       968\n",
      "         1.0       0.70      0.85      0.77       474\n",
      "\n",
      "    accuracy                           0.83      1442\n",
      "   macro avg       0.81      0.84      0.82      1442\n",
      "weighted avg       0.85      0.83      0.84      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "clf.fit(X, y)\n",
    "preds = clf.predict(X_test)\n",
    "print(classification_report(y_test, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d617341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87       968\n",
      "         1.0       0.71      0.84      0.77       474\n",
      "\n",
      "    accuracy                           0.84      1442\n",
      "   macro avg       0.81      0.84      0.82      1442\n",
      "weighted avg       0.85      0.84      0.84      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "Xr = vectorizer.fit_transform(train.raztokd) #trains the data based on the words tokenized by razdel \n",
    "X_testr = vectorizer.transform(test.raztokd)\n",
    "yr = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "\n",
    "clf = LogisticRegression(C=0.1, class_weight='balanced', max_iter =1000)\n",
    "clf.fit(Xr, yr)\n",
    "predsr = clf.predict(X_testr)\n",
    "print(classification_report(y_test, predsr, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2db3e",
   "metadata": {},
   "source": [
    "Токенизация у векторайзеров в sklearn работает лучше, чем токенизация у razdel.tokenize. Хотя разница очень маленькая, мы видим что sklearn с больше precision и выше recall опредляет, какие твиты должны быть классифицированы как \"1\" (токсичный)\n",
    "Хотя, каждый раз разница очень маленькый, и значения precision и  recall можно меняться, f1-score всегда выше когда sklearn сам токенизорует комменты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271fb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ffa9f76",
   "metadata": {},
   "source": [
    "## Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f358949",
   "metadata": {},
   "source": [
    "Преобразуйте таблицу с абсолютными частотностями в семинарской тетрадке в таблицу с tfidf значениями. (Таблица - https://i.ibb.co/r5Nc2HC/abs-bow.jpg) Формула tfidf есть в семинаре на картнике с пояснениями на английском. \n",
    "Считать нужно в питоне. Формат итоговой таблицы может быть любым, главное, чтобы был код и можно было воспроизвести вычисления. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5b50abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22314355131420976, 0.5108256237659907, 0.9162907318741551]\n",
      "[0.9162907318741551, 0.5108256237659907, 0.22314355131420976]\n",
      "[0.8925742052568391, 0.8925742052568391, 0.8925742052568391, 0.5108256237659907, 0.9162907318741551, 0.8925742052568391]\n",
      "[0.9162907318741551, 1.6094379124341003, 0.22314355131420976]\n",
      "[1.6094379124341003]\n"
     ]
    }
   ],
   "source": [
    "# ваш код\n",
    "import re \n",
    "import numpy\n",
    "strings = ['я и ты', 'ты и я', 'я я я и только я','только не я','он']\n",
    "splitstrings =[]\n",
    "for string in strings:\n",
    "    wordss = re.split(' ', string)\n",
    "    splitstrings.append(wordss)\n",
    "WOdups =[] #array without duplicates to calculate freqdict\n",
    "for i in splitstrings:\n",
    "    WOdups.append(list(dict.fromkeys(i)))\n",
    "freqdict ={}\n",
    "for i in WOdups:\n",
    "    for j in i:\n",
    "        if j in freqdict:\n",
    "            freqdict[j]+=1\n",
    "        if j not in freqdict:\n",
    "            freqdict[j]=1\n",
    "numOfDocs = len(strings)\n",
    "answerArray =[]\n",
    "for string in splitstrings:\n",
    "    toadd =[]\n",
    "    for word in string:\n",
    "        numOfTimes = 0 \n",
    "        for word2 in string: \n",
    "            if word == word2:\n",
    "                numOfTimes +=1\n",
    "        N = numpy.log(numOfDocs/freqdict[word])*numOfTimes\n",
    "        toadd.append(N)\n",
    "    answerArray.append(toadd)\n",
    "for array in answerArray:\n",
    "    print(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fdf398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f5bc8de",
   "metadata": {},
   "source": [
    "## Задание 3 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8961bbf",
   "metadata": {},
   "source": [
    "Обучите 2 любых разных классификатора из семинара. Предскажите токсичность для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов. Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46681ef",
   "metadata": {},
   "source": [
    "Требования к классификаторам:   \n",
    "а) один должен использовать CountVectorizer, другой TfidfVectorizer  \n",
    "б) у векторазера должны быть вручную заданы как минимум 5 параметров  \n",
    "в) у классификатора должно быть задано вручную как минимум 2 параметра  \n",
    "г)  f1 мера каждого из классификаторов должна быть минимум 0.75  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed77d7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                          Собаке - собачья смерть\\n    1.0\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ваш код\n",
    "dataFrame = pd.read_csv('labeled.csv')\n",
    "dataFrame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cd916f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.1, shuffle = True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6733633",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(stop_words=['он','это','она'], min_df=1, max_df= 0.4, ngram_range=[1,2],lowercase=True)\n",
    "X1 = vectorizer1.fit_transform(train.comment)\n",
    "X_test1 = vectorizer1.transform(test.comment)\n",
    "y1 = train.toxic.values\n",
    "y_test1 = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0fb95ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.83      0.85       951\n",
      "         1.0       0.70      0.77      0.73       491\n",
      "\n",
      "    accuracy                           0.81      1442\n",
      "   macro avg       0.78      0.80      0.79      1442\n",
      "weighted avg       0.81      0.81      0.81      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "clf.fit(X1, y1)\n",
    "preds1 = clf.predict(X_test1)\n",
    "print(classification_report(y_test1, preds1, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9bf6171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0\n",
      "0.9999999997993707\n",
      "Стас, никому, кроме тебя и армии твоих подсосов(которые представляют собой типичный дегенеративный биомусор, ведущийся на любые скандалы-интриги), твои ролики нахуй не нужны. Серьёзно, ты сделал новости с целью показать, что такое говно может делать любой, а аудитория осталась на том же уровне, ведь людям извне ты не интересен. Да ещё и просит не подписываться, чтобы такую-то годноту ложкой хлебать подольше. Ты обосрался, стал посмешищем для абсолютно всех ютуберов, которые не являются полными ебланами. Тот же Хованский не ссыт тебе на ебало только потому, что ты вертишься с ним в одной компании, иногда даже лично пересекаетесь. Приятно было слышать, как он говорил, что отстреливался бы от таких, как ты, из огнестрела, стараясь забрать с собой побольше коммунистов, когда они придут его оаскулачиапть? Он открыто хуесосил людей и за меньшие грехи. Сложи 2 и 2, как он к тебе относится на самом деле. После чего ты сделал ещё более смешной ролик, где истеришь как побитая шлюха во время ПМС. Я ПОДЕБИЛ, А ЕСЛИ ВЫ НЕ ПОНЯЛИ ЭТОГО, ТО ВЫ ТУПЫЕ . Ты мог хотя бы сам его посмотреть перед заливом? Мне даже рофлить над тобой расхотелось, из смешного дегенерата, ты стал жалким дурачком. Это как смеятся над роликами, где контуженные ветераны пытаются ходить под клубную музыку. Над неполноценными смеятся плохо, даже стыдно стало. Я не утрирую. Просто посмотри на себя, Стас. Ну правда. Банишь людей в группе за лвйки и одно упоминание стрима. Ты делаешь всё, в чём самый отбитый и дегенеративный либераст обвиняет совок и сверкаешь разорванным очком. Никто тебя несправедливо не обсирал. Что на стриме по поводу дат, ну ты же сам проебался. На подкасте сообщил, что не будешь стримить. Если ты не был уверен, то зачем это говорить? А если был, почему не сообщил Маргиналу сразу же? Твоё слово в целом не стоит нихуя. Обещаешь не банить-куча удалённых комментов. Обещаешь стрим-не идёшь. Обещаешь что-то ещё, всегда проёбываешься, всё чаще на нарушение обещания тебе нужно в районе секунды-дня. Стоит ли удивлятся, что тебе за это прилетело? Когда то должно было. Ты сам срёшь себе в штаны, не злись, когда на это указывают пальцем.\n",
      "\n",
      "2\n",
      "0.0\n",
      "0.9999999997993707\n",
      "Красивый, но не драгоценный\n",
      "3\n",
      "1.0\n",
      "0.9999999940228046\n",
      "смотрите, националистов, хачей, у меня, честно, свою страну, ололо, по щщам. Когда европейца, мусульман, нет, здоровая человека - араба за прав, поправьте. Поправил.\n",
      "\n",
      "4\n",
      "1.0\n",
      "0.9999999151262619\n",
      "а как по твоему происходит разгрузка товаров в магазинах? ну земля тебе пухом тогда братишка\n",
      "5\n",
      "1.0\n",
      "0.999996870356522\n",
      "Оправдываешь снова, Дешёвку Соколова? Лжеца и проходимца, Правительства любимца? Маэстре не завидуй, Ты вша, букашка, гнида. Ты можешь долго злиться, Но Женя извинится. Он не отвертится, когда Придёт решение суда. Придётся Жене зачитать, Что он любитель много врать И Соколов - красавец, А Женичка - засранец. извинится Маэстро не сломаешь, Судом не запугаешь. Не на того нарвался, Ты Сокол-петушок. Вы скоро ужаснетесь, И жидко обосрётесь, Олега Соколова, Увидев без порток. Не на того нарвался, Ты Сокол-петушок. А ты в конец заврался И с рифмой обосрался. К тому же ты совсем забыл, Что гуру ролик удалил. И в дальние он страны Готовит чемоданы. И в дальние он страны Готовит чемоданы. Не может быть двух мнений, Не будет извинений. Уйдёт не ждя ареста, Ебал вас в рот маэстро.\n",
      "\n",
      "6\n",
      "1.0\n",
      "0.9999919381565607\n",
      "Что ты несешь? ты поехавший? Ты понимаешь чем к примеру физичиеские законы отличаются от теорий разной степени охуительности ? Закон гравитации работет на всей планете Земля независимо от твоих манятеорий. Пространство блядь- трехмерно. А ты утверждаешь что оно может быть , 5,6 и прочее мерным, но доказать и продемонстрировать мы не можем по этому представим... Ты ты тупой верун Есть невидимый мужик , доказать и подтвердить мы не можем но давайте поверим... Иди нахуй. Ты настолько туп что не понимаешь даже что время- это иллюзия.\n",
      "\n",
      "7\n",
      "1.0\n",
      "0.9999911740676647\n",
      "Хлыщ ты ТС. И .... Расходимся парни, нас опять наебали.\n",
      "\n",
      "8\n",
      "0.0\n",
      "0.9999865844476205\n",
      "Ниже этой фразы в статье ведь указано про метод определения мощности: ток контроллера умножить на напряжение. К тому же бюджетные электровелосипеды не комплектуются батареями на 60в. Насчет комплекта для переделки велосипеда в электровелосипед абсолютно верно. Хочешь сделать хорошо - сделай сам.\n",
      "\n",
      "9\n",
      "0.0\n",
      "0.9999862710349318\n",
      "Меня так в три года в больничку с дистрофией клали. Тоже думали проголодается-поест . Не проголодалась, на скорой увозили через неделю. Чувство голода смогла ощущать только лет в 14-15.\n",
      "\n",
      "10\n",
      "0.0\n",
      "0.9999144019890482\n",
      "Я конечно с вами согласен, всегда любая оптимизация была выгодна для одних и невыгодна для других. В данном случае выгоду имеет только магазин. (Да, это бизнес, а не благотворительная организация. Мы по этому поводу можем только поговорить). Покупатели тоже ничего от этого не получили, т.к. затраты на этих сотрудников незначительны, чтобы повлиять на стоимость товаров.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probas = clf.predict_proba(X1)\n",
    "preds = probas[:,1]\n",
    "trainForLoop =train\n",
    "for i in range (10):\n",
    "    result = numpy.where(preds == numpy.amax(preds))\n",
    "    index = result[0][0]\n",
    "    print(i+1)\n",
    "    print(trainForLoop.toxic[index])\n",
    "    print(preds[index])\n",
    "    print(trainForLoop.comment[index])\n",
    "    preds = np.delete(preds, index)\n",
    "    trainForLoop = trainForLoop.drop(index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e29725",
   "metadata": {},
   "source": [
    "из десяти самые токсичные комменты, пять были отмечаны в оригиналном корпусе как токсичными, а пять были отмечаны как не токсичными. В основном, комменты с матерными словами были отмечены как токсичными (на пример, \"нахуй\" в первом, \"блять\" в девятом комменте) но есть и исключение. Например, второй, седьмой восьмой и десятый комменты не содерживает матерные слова, и на самом деле не токсичными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "593383e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(min_df=4, max_df=0.05, stop_words=['он', 'это', 'она', 'не', 'что'], ngram_range=[1,2], use_idf=True)\n",
    "X2 = vectorizer2.fit_transform(train.comment)\n",
    "X_test2 = vectorizer2.transform(test.comment)\n",
    "y2 = train.toxic.values\n",
    "y_test2 = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07774e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.90      0.89       951\n",
      "         1.0       0.80      0.76      0.78       491\n",
      "\n",
      "    accuracy                           0.85      1442\n",
      "   macro avg       0.84      0.83      0.83      1442\n",
      "weighted avg       0.85      0.85      0.85      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf2 = MultinomialNB(alpha =0.1, fit_prior =False)\n",
    "clf2.fit(X2, y2)\n",
    "preds2 = clf2.predict(X_test2)\n",
    "print(classification_report(y_test2, preds2, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50f6dc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0\n",
      "0.9999822864105564\n",
      "Блеаадь, как же обидно когда создаешь тред, пародируешь речь ватников, случайно употребив слово из скрипта, и он скрывается у всех. Пересоздаю. Я много лет тут сижу с вами и обсераю пидорашек хоть я и сам один из них , смеюсь над их смертями, делаю фотожабы с обезьянами. Я-то думал это делают русские русофобы и украинцы, которых процентов 10, поэтому не относился как к чему-то иному как самокритике, самоиронии или справедливому негодованию из-за войны на Донбассе (в конце концов, я считаю украинцев своими братьями и их бугурт небезосновательным). Но меня нахуй осенило когда сначала вернули значки, а потом произошел расстрел мечети в НЗ. Тут до кучи блядских муслимов и они делают это вместе с нами (я думал они слишком беспросветно тупы для того, чтобы интересоваться политикой). Называют русских пидорашками, славщитом. Бляядь, вы действительно думаете вы лучше пидорашек? И вообще славян? Хорошо, я согласен, что кавказские мужчины часто выглядят лучше русских. Но в остальном вы намного хуже даже пидорашек, которые живут окруженные говном и не охуевают, когда с утра, выходя из подъезда на работу, вступают в десятисантиметровую грязь, считая это нормой; которые не имеют никакой солидарности ни по какому признаку, ненавидят друг друга априори, даже своих детей бросают как ёбаные негры, которые скорее сдадут тебя начальству за маячащее вознаграждение (которого им не дадут), если попытаешься подбить людей на забастовку из-за невыплаты зарплаты за полгода; трусливый и терпеливый скот, который барина превозносит и делит людей на касты по признаку наличия и крутости жоповозки; пиздлявые уебки, которые превозносят свой уебищный совок, КОТОРЫЙ ИЗОБРЕЛ ВСЕ В МИРЕ А АМЕРИКАНЦЫ УКРАЛИ!!11, но по факту выясняется, что совок сам практически нихуя не построил и не модернизировал, всё либо по западным проектам с согласия запада, либо с помощью технологического шпионажа. Воистину народ-гной, народ-пидор, народ-мразь, самый подлый народ в мире. Но вы даже на фоне пидорашек выглядите микробами, вы просто ёбаные ничтожества, причем я не пытаюсь вас оскорбить, мои слова действительно отражают вложенный в них изначальный смысл, вы ничто. Да, пидорашки именно такие, какими я описал их выше, они уберхуевые, но они днище цивилизованных народов, тогда как вас к этим цивилизованным народам даже отнести нельзя. Пидорашки хоть что-то изобрели, а если не изобрели, то смогли спиздить и воссоздать технологии и не просто какую-нибудь открывалку банок, а ядерное оружие и энергетику, создали ценящуюся в мире литературу, создали пусть и хуёвую, милитаризованную, построенную на лжи и крови, угнетении и унижении собственных граждан и в конце концов развалившуюся, но сверхдержаву, сделали массу географических открытий и захватили полмира, сейчас воспитывают массу хороших кодеров (правда они скорее воспитываются не благодаря, а вопреки, когда видят какой пиздец вокруг и находят из него такой выход). Не говоря уж об остальных славянах, которых вы тут презрительно называете славщитом и чистильщиками британских туалетов. А по факту поймите, мы-то конечно говно, по сравнению со странами первого мира, с Китаем, ЮК и Японией, даже со странами Южной Америки. Но вы-то блядь даже среди рашкинских регионов самое дно. В прошлом вы (чеченцы, даги и прочие сорта) были просто дикарями, которые нападали на русские караваны и только поэтому вас решили выебать и зохватить. А если говорить о среднеазиатах, которые бугуртят с оккупации, то вы были обычными нищими кочевниками, подтиравшимися камнем и пьющими верблюжью мочу, не имеющими названий народами. Сейчас вас все боятся потому, что если пидорашка сделает в вас пару дырок, защищая себя, то его посадят на бутылку, а если вы убьёте пидорашку, то вам нихуя не будет. И есть мнение, что это сделано специально, чтобы русский воспитывался униженным рабом. Вы унижаете русских детей в школе, потому что вы агрессивные обезьяны, не способные в диалог и возгорающиеся с малейшей искры, и пусть мне тут кто-нибудь заикнется про протопоповскую хуету, дети в школе должны учиться, вдохновляться и выбирать будущую профессию, а не бороться за выживание и дебильный статус в иерархии. И не слушайте дебилов из b , унижение и избиение слабых не делает вас лучше тех, кого вы унижаете, делает защита. Я не люблю выражаться фразами нациков, но вы воистину столетия просто сидели в горах и ебали баранов, вас даже народ-пидор смог захватить. Я знаю, что есть много армянских ученых, музыкантов, известных людей всех сортов, получивших признание в странах первого мира. Я знаю про охуенную грузинскую культуру (которую вы, судя по всему, нагло спиздили в свое время). Но где блядь чеченские я знаю, что вас тут особенно много, поэтому акцентирую внимание именно на вас среди всех муслимских народов СНГ , дагестанские ученые? Открыли дверь? Где государства, культуры? Почему в среднеазиатских парашах работы нет вообще и они все едут уже не в рашку, а в азиатские страны работать заграницу? Хоть что-то есть в вас достойное упоминания, кроме наглости и агрессии? Арийцы, блядь? Самая чистая раса, о чем любят похвастаться чеченцы? Пик 2, вот такие арийцы. Помесь арабов турков, славян и других европеоидов. Алсо, Володин долбит, Пыня хуйло, рашка бантустан, русня народ-гной.\n",
      "\n",
      "2\n",
      "1.0\n",
      "0.9999760729109727\n",
      "Какие же хохлы незалежные дегенераты, пиздец просто.\n",
      "\n",
      "3\n",
      "0.0\n",
      "0.9999749564190287\n",
      "О, какой классный дизайн. Первый раз вижу дизайн, практически бех изъяна\n",
      "\n",
      "4\n",
      "1.0\n",
      "0.9999671670005595\n",
      "Хлыщ ты ТС. И .... Расходимся парни, нас опять наебали.\n",
      "\n",
      "5\n",
      "1.0\n",
      "0.9999609029630956\n",
      "Вот вам и безвиз. Какие же хохлы дегенераты, пиздец просто\n",
      "\n",
      "6\n",
      "1.0\n",
      "0.9999590048809809\n",
      "Совершенно верно. Крышняк едет ого-го.\n",
      "\n",
      "7\n",
      "0.0\n",
      "0.9999536181212353\n",
      "В США тащемто не прямая демократия)\n",
      "\n",
      "8\n",
      "0.0\n",
      "0.9999536181212353\n",
      "Не уверен. Я не так много работаю в этом месте. Но в прошлый мой заход было лучше, пару лет назад. Ну а так, периодически...\n",
      "\n",
      "9\n",
      "1.0\n",
      "0.9999308161267783\n",
      "Не, по феменистски это когда бабы-страшилы снимаются, так что не прокатит в данном случае)\n",
      "\n",
      "10\n",
      "0.0\n",
      "0.9999304375781168\n",
      "Нет, тут как раз случай когда цена ровно та что должна быть, а не наебалово.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probas2 = clf2.predict_proba(X2)\n",
    "preds2 = probas2[:,1]\n",
    "trainForLoop2 =train\n",
    "for i in range (10):\n",
    "    result = numpy.where(preds2 == numpy.amax(preds2))\n",
    "    index = result[0][0]\n",
    "    print(i+1)\n",
    "    print(trainForLoop2.toxic[index])\n",
    "    print(preds2[index])\n",
    "    print(trainForLoop2.comment[index])\n",
    "    preds2 = np.delete(preds2, index)\n",
    "    trainForLoop2 = trainForLoop2.drop(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e73d04",
   "metadata": {},
   "source": [
    "Топ десятые комменты почти не совпадают с токсичными комментами прошлого классифакатора. Только коммент начиная с \"Какие блять передергивания?\" совпадает. Здесь тоже много матерные слова, но как в прошлом списке, есть исключение, где не понятно почему эти комменты может быть токсичными. Например, на самом деле, 4-й, 6-й и 9-й комменты не токсичными. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68324753",
   "metadata": {},
   "source": [
    "## *Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7794f97",
   "metadata": {},
   "source": [
    "Для классификаторов LogisticRegression, Decision Trees, Naive Bayes, Random Forest найдите способ извлечь важность признаков для предсказания токсичного класса. Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов. \n",
    "\n",
    "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621065d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b0128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba358e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
