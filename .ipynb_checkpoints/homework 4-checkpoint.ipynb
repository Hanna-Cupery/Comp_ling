{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5440db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open('lenta.txt', encoding='utf8').read()\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "norm_news = normalize(news)\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24039"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) +['<end>'] for text in sent_tokenize(news)]\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "i = 0\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, 3))\n",
    "    if i >5000:\n",
    "        break\n",
    "    i+=1\n",
    "\n",
    "len(unigrams_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70996b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75188"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigrams_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d5f559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start>', 5002),\n",
       " ('<start> в', 450),\n",
       " ('<start> по', 393),\n",
       " ('<start> как', 274),\n",
       " ('риа новости', 171),\n",
       " ('<start> на', 126),\n",
       " ('по словам', 126),\n",
       " ('в москве', 110),\n",
       " ('что в', 108),\n",
       " ('об этом', 107)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_news.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319c0a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start> в', 450),\n",
       " ('<start> <start> по', 393),\n",
       " ('<start> <start> как', 274),\n",
       " ('<start> <start> на', 126),\n",
       " ('<start> <start> об', 99),\n",
       " ('<start> об этом', 95),\n",
       " ('<start> по словам', 94),\n",
       " ('<start> <start> однако', 94),\n",
       " ('<start> как сообщает', 88),\n",
       " ('<start> <start> он', 77)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91af4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = np.zeros((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "id2word_news_unigrams = list(unigrams_news)\n",
    "word2id_news_unigrams = {word:i for i, word in enumerate(id2word_news_unigrams)}\n",
    "id2word_news_bigrams = list(bigrams_news)\n",
    "word2id_news_bigrams = {word:i for i, word in enumerate(id2word_news_bigrams)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda6d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_news[word2id_news_bigrams[bigram]][word2id_news_unigrams[word3]]= (trigrams_news[ngram]/bigrams_news[bigram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dfc6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word_unigram, word2id_unigram, id2word_bigram, word2id_bigram, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = word2id_bigram[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        bigram = id2word_bigram[current_idx]\n",
    "        first_word, second_word = bigram.split()\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        chosen_word = id2word_unigram[chosen]\n",
    "        text.append(chosen_word)\n",
    "        if chosen_word == '<end>':\n",
    "            current_idx = word2id_bigram['<start> <start>']\n",
    "        else:\n",
    "            current_idx = word2id_bigram[second_word + ' '+ chosen_word]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2d1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(matrix_news, id2word_news_unigrams, word2id_news_unigrams, id2word_news_bigrams, word2id_news_bigrams, n=100).replace('<end>', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc84107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в мосгоризбиркоме сообщили что руководители этих подразделений будут привлечены 83 курсанта академии гражданской защиты мчс рф генерал-майор станислав суанов \n",
      " необходимые приказы я уже отдал \n",
      " однако многие афиняне провели эту ночь гражданские добровольцы и солдаты специально подготовленные для ведения в стране \n",
      " этот вопрос и обсудить проблему на месте взрыва директор фсб рф генерал александр зданович \n",
      " во временном пресс-центре минобороны рф в центре управления полетами спутники успешно выведены на круговую орбиту \n",
      " на большейчасти европейской территории россии в том числе с правоохранительными органами и требуют принятия соответствующих мер для остановки насилия в тиморе \n",
      " предполагается что под нынешнее\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26e9a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_2 = generate(matrix_news, id2word_news_unigrams, word2id_news_unigrams, id2word_news_bigrams, word2id_news_bigrams, n=100).replace('<end>', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926e39ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "так продолжалось до июня 1996 года при попытке прорыва из окруженияиз селения чабанмахи \n",
      " специалистами минздрава подготовлен проект распоряжения правительства по конкретным мерам в связи с открытием джессопа что если его брата абдуллу все-таки казнят возможна ответная реакция курдов \n",
      " это не будет предъявлено никаких преследовавшим мерседес дианы на мотоциклах \n",
      " конвойные « помогли » ему подняться и вытянуться во фронт \n",
      " однако судья все же выехали сотрудники милиции и сотрудников комбината из-за опасений что взрывчатка найдена в ходе ракетно-бомбовых ударов и других действий уничтожено до 30 рублей за тонну что соответствует подорожанию на 23 по сравнению с недавним землетрясением\n"
     ]
    }
   ],
   "source": [
    "print(generated_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "011a51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_3 = generate(matrix_news, id2word_news_unigrams, word2id_news_unigrams, id2word_news_bigrams, word2id_news_bigrams, n=100).replace('<end>', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5482243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "восточный тимор \n",
      " на тайвань уже прибыли 16 спасателей района сянган гонконг китая \n",
      " по мнению начальника отдела управления федеральных таможенных доходов гтк александра воронина таким образом брюссельцы отметят праздник мальчика писа \n",
      " члены бригады главарь дмитрий гуфельд александркуценко игорь котов андрей якубов александр кройтор и лоренсомуссо орудовали в нью-йоркских районах brooklyn и queens гдепроживают большие русскоязычные общины \n",
      " по мнению некоторых комментаторов само расследование было затеяно для того чтобы слушатели поняли для чего мы обращаем так много внимания на это обвиняемые рассчитывают на снисхождение судьи \n",
      " в настоящее время индонезия имеет твердое намерение сама взять под соответствующий контроль места\n"
     ]
    }
   ],
   "source": [
    "print(generated_text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3d792dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8591729",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_generated_text= normalize(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f91104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_generated_text = Counter(norm_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f263499",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_generated_text = Counter({word:c/len(generated_text) for word, c in vocab_generated_text.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88cfa777",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = {'generated_text':[]}\n",
    "\n",
    "for word in normalize(generated_text):\n",
    "    \n",
    "    if word in probas_generated_text:\n",
    "        prob['generated_text'].append(np.log(probas_generated_text[word]))\n",
    "    else:\n",
    "        prob['generated_text'].append(np.log(1/len(norm_generated_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dce25a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575.2389578510736"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(prob['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df0d2e",
   "metadata": {},
   "source": [
    "Есть два способы, оба с тэгом <UNK> (unknown). \n",
    "Первый способ принимается когда изначально есть список правильных слов. В процессе нормализации, слова, которых нет в списке заменяются на тэг <UNK>, и потом тэг вычисляется как другие слова. \n",
    "Второй способ принимается когда нет списка правильных слов изначально. В этом методе, самые нечастые слова заменяются на тэг <UNK>, и как в первом методе, тэг вычисляется как другие слова. Можно или заменять такие слова, которые встречаются меньше n раз, или решить что словарь состоится от V слов, и не заменять самые частные V слова, а заменять остальные.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c3701",
   "metadata": {},
   "source": [
    "Сглаживание это процесс, который дает маленькая вероятность к случаям, которых не были в тренировочных данных. Это делается потому что, такие случаи могут встречаться в тестовых данных. \n",
    "Есть разные способы реализовать сглаживание. Например, в сглаживании Лапласа, каждому случаю добавляется один. Но это дает слишком много вероятности нулям, и поэтому можно добавить число, меньше одного, как 0.5 или 0.1. Ещё можно вычислять вероятность используя более маленькие ngram (например, если вычисляете вероятность триграмы, добавить вероятность биграм или униграм) Ещё можно вычитать какое-то абсолютное значение, как в Kneser-Nay. Ещё можно вычислять вероятность того, что слова находится в новом контексте, на основании того, в скольких разных контекстах это слово было. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
