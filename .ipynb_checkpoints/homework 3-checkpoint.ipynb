{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be25c",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9861e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196bee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('wiki_data.txt', encoding='utf8').read()\n",
    "vocab = Counter(re.findall('\\w+', corpus.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538ecd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=1000)\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91e6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    \n",
    "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
    "    # 1 вектора ко всей матрице (словам в словаре)\n",
    "    # считать по отдельности циклом было бы дольше\n",
    "    # вместо одного вектора может даже целая матрица\n",
    "    # тогда считаться в итоге будет ещё быстрее\n",
    "    \n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "    # Counter можно использовать и с не целыми числами\n",
    "    similarities = Counter()\n",
    "    \n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(topn)\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "\n",
    "    \n",
    "    return closest\n",
    "\n",
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return vocab[word] / N\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eaa5d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "знать\n"
     ]
    }
   ],
   "source": [
    "# ваш код тут\n",
    "def find_most_likely_word(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    possible_words = get_closest_hybrid_match(text, X, vec, topn, metric)\n",
    "    probability_of_words ={}\n",
    "    for word in possible_words:\n",
    "        probability_of_words[word[0]] = P(word[0])\n",
    "    most_likely_word = max(probability_of_words, key=probability_of_words.get)\n",
    "    return(most_likely_word)\n",
    "    \n",
    "\n",
    "print(find_most_likely_word('знають', X, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам применяется только одна операция - удаление символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово   \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00fbb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c9fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код тут\n",
    "corpus = open('wiki_data.txt', encoding='utf8').read()\n",
    "tokens = corpus.lower().split()\n",
    "tokens = [token.strip(punctuation) for token in tokens]\n",
    "tokens = [token for token in tokens if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "666be7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_with_frequencies ={}\n",
    "for token in tokens:\n",
    "    if (str(token) in tokens_with_frequencies):\n",
    "        tokens_with_frequencies[token] = tokens_with_frequencies.get(token)+1\n",
    "    else:\n",
    "        tokens_with_frequencies[token] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7da0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#На основе словаря правильных слов составляется словарь удалений - \n",
    "#для каждого правильного слова создаются все варианты удалений и создается словарь,\n",
    "#где ключ - слово с удалением, а значение - правильное слово\n",
    "wrong_words ={}\n",
    "for word in tokens_with_frequencies.keys():\n",
    "    for i in range(len(word)):\n",
    "        wrong_word = word[0:i]+word[i+1:len(word)]\n",
    "        wrong_words[wrong_word] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c81928e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N= len(tokens_with_frequencies)\n",
    "def P(word):\n",
    "    if word in tokens_with_frequencies:\n",
    "        return tokens_with_frequencies[word]/N\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fbef6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "setn ={2}\n",
    "if len(setn)==1:\n",
    "    iterator = iter(setn)\n",
    "    item1 = next(iterator, None)\n",
    "    print(item1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d9020ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лбдымвоывидют\n"
     ]
    }
   ],
   "source": [
    "#Для выбора исправления для слова с опечаткой генерируются все варианты удаления, \n",
    "#из них выбираются те, что есть в словаре удалений, построенного на шаге 2.\n",
    "#Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления\n",
    "\n",
    "def find_correct_word(word):\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    words_with_deletions    = [L + R[1:]               for L, R in splits if R]\n",
    "    possible_words = set(words_with_deletions) & set(wrong_words)\n",
    "    if len(possible_words) ==0:\n",
    "        return word\n",
    "    else: \n",
    "        return max(possible_words, key=P)\n",
    "correct_word = find_correct_word('лбдымвоывидют')\n",
    "print(correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef91de75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Он устал после работы случается что его обижали большие мальчишки в суровом бизнесе и ругал злой дядька-начальник'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
    "bad.pop(778)\n",
    "true.pop(778)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "851dd7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca53478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
    "        # перед тем как считать исправление проверим нет ли его в кеше\n",
    "        \n",
    "        predicted = cashed.get(pair[1], find_correct_word(pair[1]))\n",
    "        cashed[pair[1]] = predicted\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "        \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024745f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e249226",
   "metadata": {},
   "source": [
    "Этот метод работает хуже по всем метрикам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "find_correct_word('солнвце')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c2f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "find_correct_word('насмехатьсяаававттававаываываы')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292d96d",
   "metadata": {},
   "source": [
    "## *3. Настройка гиперпараметров. (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4b28f",
   "metadata": {},
   "source": [
    "У метода из первого заданий много гиперпараметров (те которые нужно подбирать самостоятельно). Это параметры векторайзера, topn, метрика редактирования. Поэкспериментируйте с ними. \n",
    "\n",
    "Проведите как минимум 10 экспериментов с разными параметрами. Для каждого эксперимента укажите мотивацию (например, \"слишком маленький topn в get_closest_match_vec приводит к тому, что некоторые хорошие варианты не доходят до get_closest_match_with_metric, попробуем его увеличить\")\n",
    "\n",
    "Старайтесь получить улучшение, но если улучшить не получится, то это не страшно. Главное, чтобы эксперименты были осмысленными, а не рандомными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a435a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389eba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
