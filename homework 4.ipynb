{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5440db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open('lenta.txt', encoding='utf8').read()\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "norm_news = normalize(news)\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24039"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) +['<end>'] for text in sent_tokenize(news)]\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "i = 0\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, 3))\n",
    "    if i >5000:\n",
    "        break\n",
    "    i+=1\n",
    "\n",
    "len(unigrams_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70996b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75188"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigrams_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d5f559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start>', 5002),\n",
       " ('<start> в', 450),\n",
       " ('<start> по', 393),\n",
       " ('<start> как', 274),\n",
       " ('риа новости', 171),\n",
       " ('<start> на', 126),\n",
       " ('по словам', 126),\n",
       " ('в москве', 110),\n",
       " ('что в', 108),\n",
       " ('об этом', 107)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_news.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319c0a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start> в', 450),\n",
       " ('<start> <start> по', 393),\n",
       " ('<start> <start> как', 274),\n",
       " ('<start> <start> на', 126),\n",
       " ('<start> <start> об', 99),\n",
       " ('<start> об этом', 95),\n",
       " ('<start> по словам', 94),\n",
       " ('<start> <start> однако', 94),\n",
       " ('<start> как сообщает', 88),\n",
       " ('<start> <start> он', 77)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91af4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = np.zeros((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "id2word_news_unigrams = list(unigrams_news)\n",
    "word2id_news_unigrams = {word:i for i, word in enumerate(id2word_news_unigrams)}\n",
    "id2word_news_bigrams = list(bigrams_news)\n",
    "word2id_news_bigrams = {word:i for i, word in enumerate(id2word_news_bigrams)}\n",
    "id2word_news_trigrams = list(trigrams_news)\n",
    "word2id_news_trigrams = {word:i for i, word in enumerate(id2word_news_trigrams)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda6d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_news[word2id_news_bigrams[bigram]][word2id_news_unigrams[word3]]= (trigrams_news[ngram]/bigrams_news[bigram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dfc6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word_unigram, word2id_unigram, id2word_bigram, word2id_bigram, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = word2id_bigram[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        bigram = id2word_bigram[current_idx]\n",
    "        first_word, second_word = bigram.split()\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
    "        chosen_word = id2word_unigram[chosen]\n",
    "        text.append(chosen_word)\n",
    "        if chosen_word == '<end>':\n",
    "            current_idx = word2id_bigram['<start> <start>']\n",
    "        else:\n",
    "            current_idx = word2id_bigram[second_word + ' '+ chosen_word]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e2d1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(matrix_news, id2word_news_unigrams, word2id_news_unigrams, id2word_news_bigrams, word2id_news_bigrams, n=100).replace('<end>', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc84107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "к террористической группе осуществившей взрывы жилых домов в столице карачаево-черкесии городе черкесске \n",
      " в этой связи юрий лужков и впредь \n",
      " властям удалось взять ситуацию под контролем обсе \n",
      " оно ощущалось как на сопровождаемый так и не локализован \n",
      " на прошлой неделе была найдена запись на которой агенты фбр признали что использовали в уэйко и найти того кто не побоится нас защитить \n",
      " специалисты гостелекома отмечают что новый блок первое объединение инициатива создания которого исходит непосредственно из регионов \n",
      " тогда же прибыли венки от президента и возглавил орт \n",
      " последние события в дагестане террористические акты в двух местных кафе \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3d792dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8591729",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_generated_text= normalize(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f91104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_generated_text = Counter(norm_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f263499",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_generated_text = Counter({word:c/len(generated_text) for word, c in vocab_generated_text.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88cfa777",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = {'generated_text':[]}\n",
    "\n",
    "for word in normalize(generated_text):\n",
    "    \n",
    "    if word in probas_generated_text:\n",
    "        prob['generated_text'].append(np.log(probas_generated_text[word]))\n",
    "    else:\n",
    "        prob['generated_text'].append(np.log(1/len(norm_generated_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dce25a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528.1620154711247"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(prob['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867651ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df0d2e",
   "metadata": {},
   "source": [
    "Есть два способы, оба с тэгом <UNK> (unknown). \n",
    "Первый способ принимается когда изначально есть список правильных слов. В процессе нормализации, слова, которых нет в списке заменяются на тэг <UNK>, и потом тэг вычисляется как другие слова. \n",
    "Второй способ принимается когда нет списка правильных слов изначально. В этом методе, самые нечастые слова заменяются на тэг <UNK>, и как в первом методе, тэг вычисляется как другие слова. Можно или заменять такие слова, которые встречаются меньше n раз, или решить что словарь состоится от V слов, и не заменять самые частные V слова, а заменять остальные.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c3701",
   "metadata": {},
   "source": [
    "Сглаживание это процесс, который дает маленькая вероятность к случаям, которых не были в тренировочных данных. Это делается потому что, такие случаи могут встречаться в тестовых данных. \n",
    "Есть разные способы реализовать сглаживание. Например, в сглаживании Лапласа, каждому случаю добавляется один. Но это дает слишком много вероятности нулям, и поэтому можно добавить число, меньше одного, как 0.5 или 0.1. Ещё можно вычислять вероятность используя более маленькие ngram (например, если вычисляете вероятность триграмы, добавить вероятность биграм или униграм) Ещё можно вычитать какое-то абсолютное значение, как в Kneser-Nay. Ещё можно вычислять вероятность того, что слова находится в новом контексте, на основании того, в скольких разных контекстах это слово было. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
